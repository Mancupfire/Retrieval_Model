{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Nhat Minh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('edinburgh-keywords_train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "keywords = list(train_data['np2count'].keys())\n",
    "\n",
    "keyword_set = set(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_users(info):\n",
    "    l_user, user2kw = [], []\n",
    "    for ii in info:\n",
    "        lus = info[ii]\n",
    "        for u in lus:\n",
    "            if u not in l_user:\n",
    "                l_user.append(u)\n",
    "                user2kw.append([])\n",
    "            idx = l_user.index(u)\n",
    "            user2kw[idx].append(ii)\n",
    "    return l_user, user2kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users, train_users2kw = extract_users(train_data['np2users'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_set = set()\n",
    "listres = []\n",
    "for kw in train_data['np2rests'].keys():\n",
    "    listres.extend(train_data['np2rests'][kw].keys())\n",
    "restaurant_set = set(listres)\n",
    "\n",
    "keyword_set = list(keyword_set)\n",
    "restaurant_set = list(restaurant_set)\n",
    "restaurants = len(listres)\n",
    "num_keywords = len(keyword_set)\n",
    "num_restaurants = len(restaurant_set)\n",
    "a = np.zeros((num_keywords, num_restaurants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kw in train_data['np2rests'].keys():\n",
    "    for res in train_data['np2rests'][kw].keys():\n",
    "        idx_kw = keyword_set.index(kw)\n",
    "        idx_res = restaurant_set.index(res)\n",
    "        a[idx_kw][idx_res] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_embeddings = model.encode(list(keyword_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('edinburgh-keywords_test.json', 'r') as r:\n",
    "    test_data = json.load(r)\n",
    "\n",
    "user_keywords = list(test_data['np2reviews'].keys())\n",
    "user_keywords_list = list(user_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users, test_users2kw = extract_users(test_data['np2users'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_keywords = [kw for sublist in test_users2kw for kw in sublist]\n",
    "test_keyword_embeddings = model.encode(test_keywords)\n",
    "\n",
    "similarity_scores = cosine_similarity(test_keyword_embeddings, keyword_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_keywords = []\n",
    "for i, user_kw in enumerate(test_users2kw):\n",
    "    updated_user_kw = []\n",
    "    for kw in user_kw:\n",
    "        if kw not in keyword_set:\n",
    "            test_idx = test_keywords.index(kw)\n",
    "            sim_scores = similarity_scores[test_idx]\n",
    "\n",
    "            best_match_idx = np.argmax(sim_scores)\n",
    "            best_match_keyword = keyword_set[best_match_idx]\n",
    "\n",
    "            updated_user_kw.append(best_match_keyword)\n",
    "        else:\n",
    "            updated_user_kw.append(kw)\n",
    "\n",
    "    filtered_keywords.append(updated_user_kw)\n",
    "\n",
    "test_users2kw = filtered_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for kw in test_users2kw:\n",
    "    t = np.zeros((1, len(keyword_set)))\n",
    "    keywords = kw[:10]\n",
    "    for keys in keywords:\n",
    "        if keys in keyword_set:\n",
    "            idx_kw = keyword_set.index(keys)\n",
    "            t[0][idx_kw] = 1\n",
    "    R = np.dot(t, a)\n",
    "    result = np.argsort(R[0])[::-1][:10]\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#         for i, result in enumerate(results):\n",
    "#             restaurant_names = [restaurant_set[idx] for idx in result]\n",
    "#             print(f\"The result for user {i} is: {restaurant_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SỬ DỤNG LLM ĐỂ RE-RANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mre_rank_candidates\u001b[39m(user_id, candidate_restaurants, user_keywords):\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    Sử dụng LLM để re-rank các candidate restaurants dựa trên từ khóa của người dùng.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m        list: Danh sách restaurant được xếp hạng lại.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "llm_pipeline = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "def re_rank_candidates(user_id, candidate_restaurants, user_keywords):\n",
    "    \"\"\"\n",
    "    Sử dụng LLM để re-rank các candidate restaurants dựa trên từ khóa của người dùng.\n",
    "\n",
    "    Args:\n",
    "        user_id (str): Mã định danh của người dùng.\n",
    "        candidate_restaurants (list): Danh sách tên restaurant được candidate.\n",
    "        user_keywords (list): Danh sách từ khóa liên quan đến sở thích của người dùng.\n",
    "\n",
    "    Returns:\n",
    "        list: Danh sách restaurant được xếp hạng lại.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Người dùng có sở thích: {', '.join(user_keywords[:5])}. \"  \n",
    "        f\"Với các candidate restaurants sau: {', '.join(candidate_restaurants[:5])}. \"  \n",
    "        \"Hãy xếp hạng lại các restaurant theo mức độ phù hợp với sở thích của người dùng theo thứ tự giảm dần và chỉ in ra danh sách tên các restaurant, cách nhau bằng dấu phẩy.\"\n",
    "    )\n",
    "    \n",
    "    generated = llm_pipeline(prompt, max_new_tokens=200, do_sample=False)[0]['generated_text']\n",
    "    \n",
    "    output = generated[len(prompt):].strip()\n",
    "    \n",
    "    re_ranked = [restaurant.strip() for restaurant in output.split(',')]\n",
    "    \n",
    "    valid_re_ranked = [r for r in re_ranked if r in candidate_restaurants]\n",
    "    if len(valid_re_ranked) == 0:\n",
    "        valid_re_ranked = candidate_restaurants\n",
    "    return valid_re_ranked\n",
    "\n",
    "final_results = []\n",
    "for idx, (user, candidate_indices) in enumerate(zip(test_users, results)):\n",
    "    candidate_restaurants = [restaurant_set[i] for i in candidate_indices]\n",
    "    user_kw = test_users2kw[idx]\n",
    "    re_ranked = re_rank_candidates(user, candidate_restaurants, user_kw)\n",
    "    final_results.append(re_ranked)\n",
    "    print(f\"Re-ranked results for user {user}: {re_ranked}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to: ./result/results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "output_data = {}\n",
    "\n",
    "for idx, (user, restaurant_indices) in enumerate(zip(test_users, results)):\n",
    "    user_data = {}\n",
    "    \n",
    "    user_keywords = test_users2kw[idx]\n",
    "    \n",
    "    candidate_restaurants = [restaurant_set[i] for i in restaurant_indices]\n",
    "    \n",
    "    positions = [str(i) for i in restaurant_indices]\n",
    "\n",
    "    re_ranked = re_rank_candidates(user, candidate_restaurants, user_keywords)\n",
    "    \n",
    "    user_data[\"kw\"] = user_keywords[:10]\n",
    "    user_data[\"candidate\"] = re_ranked[:10]\n",
    "    user_data[\"positions\"] = positions[:10]\n",
    "    \n",
    "    output_data[user] = user_data\n",
    "\n",
    "json_file_path = './result/results.json'\n",
    "\n",
    "with open(json_file_path, mode=\"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(output_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\nResults saved to: {json_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: ./result/results(afterLLM).json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "output_data = {}\n",
    "\n",
    "for idx, (user, restaurant_indices) in enumerate(zip(test_users, results)):\n",
    "    user_data = {}\n",
    "\n",
    "    user_keywords = test_users2kw[idx]\n",
    "\n",
    "    candidate_restaurants = [restaurant_set[i] for i in restaurant_indices]\n",
    "\n",
    "    re_ranked_restaurants = re_ranked\n",
    "\n",
    "    positions = [str(i) for i in restaurant_indices]  # 1-based index\n",
    "\n",
    "    user_data[\"kw\"] = user_keywords[:10]  \n",
    "    user_data[\"candidate\"] = re_ranked_restaurants[:10] \n",
    "    user_data[\"positions\"] = positions[:10]  \n",
    "    output_data[user] = user_data\n",
    "\n",
    "file_path='./result/results(afterLLM).json'\n",
    "\n",
    "with open(file_path, mode=\"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(output_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Results saved to: {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
