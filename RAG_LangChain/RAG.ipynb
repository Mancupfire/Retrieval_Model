{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ragstack-ai 0.7.0 requires langchain==0.1.4, but you have langchain 0.3.15 which is incompatible.\n",
      "ragstack-ai 0.7.0 requires langchain-community==0.0.16, but you have langchain-community 0.3.15 which is incompatible.\n",
      "ragstack-ai 0.7.0 requires langchain-core==0.1.16, but you have langchain-core 0.3.31 which is incompatible.\n",
      "ragstack-ai 0.7.0 requires langchain-openai==0.0.3, but you have langchain-openai 0.3.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-mongodb pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langgraph langchain-community beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain import hub\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langchain_community.document_loaders.web_base import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import bs4\n",
    "from pymongo import MongoClient\n",
    "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
    "from pymongo.errors import BulkWriteError\n",
    "import uuid\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Choosing llm model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Choosing text embedding model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MongoDB collection and index name\n",
    "MONGODB_COLLECTION = \"users\"\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"NEU_Team\"\n",
    "\n",
    "# Connect to MongoDB Atlas\n",
    "client = MongoClient(\"mongodb+srv://Nhatminh1810:Rich%40rd1810@cluster0.ucynl.mongodb.net/\")\n",
    "db = client[\"sample_mflix\"]\n",
    "collection = db[MONGODB_COLLECTION] \n",
    "\n",
    "# Initialize the Vector Store\n",
    "vector_store = MongoDBAtlasVectorSearch(\n",
    "    embedding=embeddings,\n",
    "    collection=collection,              \n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,  \n",
    "    relevance_score_fn=\"cosine\"         \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 10088\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://viblo.asia/p/fine-tuning-mot-cach-hieu-qua-va-than-thien-voi-phan-cung-adapters-va-lora-5pPLkj3eJRZ\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ocs[0].page_content)\n",
    "# print(docs)\n",
    "# print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 64 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=250,  # chunk size (characters)\n",
    "    chunk_overlap=50,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ObjectId('679465a3eba9995bf5cce2b1'), ObjectId('679465a3eba9995bf5cce2b2'), ObjectId('679465a3eba9995bf5cce2b3')]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    document_ids = vector_store.add_documents(documents=all_splits)\n",
    "    print(document_ids[:3])\n",
    "except BulkWriteError as bwe:\n",
    "    print(\"Bulk Write Error:\", bwe.details)\n",
    "\n",
    "for doc in all_splits:\n",
    "    doc.metadata[\"_id\"] = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: (question goes here) \n",
      "Context: (context goes here) \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "assert len(example_messages) == 1\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieve\n",
      "Retrieve information related to a query.\n",
      "{'query': {'title': 'Query', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "# Inspect cái tool trên\n",
    "print(retrieve.name)\n",
    "print(retrieve.description)\n",
    "print(retrieve.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building our System Message using ToolNode\n",
    "\n",
    "# Tạo phản hồi từ LLM có khả năng sử dụng retrieve \n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])  # Sử dụng công cụ bổ trợ retrieve mà mình đã tạo ở trên\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "\n",
    "# Khởi tạo câu trả lời sử dụng retrieve content\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]  # Danh sách được đảo ngược lại để giữ nguyên giá trị ban đầu\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"Hello\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is Fine-Tune?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (call_8OpnRnONnlzjz97lHAAOKkUw)\n",
      " Call ID: call_8OpnRnONnlzjz97lHAAOKkUw\n",
      "  Args:\n",
      "    query: Fine-Tune\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Fine-tuning is a process in machine learning where a pre-trained model is adjusted on a specific task or dataset to improve its performance. This involves continuing the training of the model with a smaller learning rate and often using a more targeted dataset. Fine-tuning helps leverage the knowledge the model gained during its initial training while adapting it to new, specific requirements.\n"
     ]
    }
   ],
   "source": [
    "input_message = \"What is Fine-Tune?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
